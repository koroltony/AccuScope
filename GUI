import tkinter as tk
from tkinter import filedialog, ttk
import cv2
from PIL import Image, ImageTk
import numpy as np
import time

class ErrorDetectionApp:
    def __init__(self, root):
        self.root = root
        self.root.title("Error Detection")
        self.root.geometry("800x600")

        # Variables for video length, progress, and control
        self.video_length = 0
        self.processed_time = 0  # Number of frames processed
        self.progress_percentage = 0
        self.video_path = None
        self.detection_paused = False
        self.cap = None  # VideoCapture object
        self.fps = 1  # Frames per second (default to 1 until updated)
        self.in_black_frame = False  # Flag to detect consecutive black frames
        self.first_black_frame_timestamp = None  # Store timestamp of the first black frame
        self.previous_frame = None
        
        # Layout
        self.create_widgets()

        # Flags for error tracking
        self.in_black_frame = False
        self.first_black_frame_timestamp = None
        
        # Store all errors for search functionality
        self.all_errors = []

    def create_widgets(self):
        # Video Playback Area
        video_frame = tk.Frame(self.root, bg="black", width=400, height=300)
        video_frame.pack(side=tk.LEFT, padx=10, pady=10, fill=tk.BOTH, expand=True)

        self.video_label = tk.Label(video_frame, bg="black")  # Video display
        self.video_label.place(relx=0.5, rely=0.5, anchor=tk.CENTER)

        # Placeholder message label (for "Please upload video" message)
        self.video_message_label = tk.Label(
            video_frame, text="Please upload video to start detection.",
            fg="white", bg="black", font=("Arial", 12)
        )
        self.video_message_label.place(relx=0.5, rely=0.5, anchor=tk.CENTER)  # Center it in the frame

        # Progress Bar
        progress_frame = tk.Frame(video_frame, bg="black")
        progress_frame.place(relx=0.5, rely=0.9, anchor=tk.CENTER)

        self.progress_label = tk.Label(progress_frame, text="Progress: 00:00:00/00:00:00 (0%)", fg="white", bg="black")
        self.progress_label.pack()

        self.progress_bar = ttk.Progressbar(progress_frame, orient="horizontal", length=300, mode="determinate")
        self.progress_bar.pack()

        # Error Details Area
        error_frame = tk.Frame(self.root)
        error_frame.pack(side=tk.RIGHT, padx=10, pady=10, fill=tk.BOTH, expand=True)

        error_label = tk.Label(error_frame, text="Detected Errors", font=("Arial", 14))
        error_label.pack(pady=5)

        # Search Bar
        search_frame = tk.Frame(error_frame)
        search_frame.pack(pady=5)

        self.search_entry = tk.Entry(search_frame, width=20)
        self.search_entry.pack(side=tk.LEFT, padx=5)

        search_button = tk.Button(search_frame, text="Search", command=self.search_errors)
        search_button.pack(side=tk.LEFT)

        # Treeview for displaying errors
        self.error_list = ttk.Treeview(
            error_frame, columns=("Timestamp", "Error Type"), show="headings", height=15
        )
        self.error_list.heading("Timestamp", text="Timestamp")
        self.error_list.heading("Error Type", text="Error Type")
        self.error_list.column("Timestamp", width=120)
        self.error_list.column("Error Type", width=200)
        self.error_list.pack(expand=True, fill=tk.BOTH)

        # Control Buttons arranged vertically
        control_frame = tk.Frame(self.root)
        control_frame.pack(side=tk.LEFT, padx=10, pady=10)

        upload_button = tk.Button(control_frame, text="Upload Video", command=self.upload_video, width=15)
        upload_button.pack(pady=5)

        start_button = tk.Button(control_frame, text="Start Detection", command=self.start_detection, width=15)
        start_button.pack(pady=5)

        pause_button = tk.Button(control_frame, text="Pause/Resume", command=self.pause_resume_detection, width=15)
        pause_button.pack(pady=5)

        end_button = tk.Button(control_frame, text="End Detection", command=self.end_detection, width=15)
        end_button.pack(pady=5)



    def upload_video(self):
        file_path = filedialog.askopenfilename(filetypes=[("Video Files", "*.mp4;*.avi;*.mov")])

        if not file_path:
            print("No file selected.")
            return  

        # Clear detected errors list and UI
        self.error_list.delete(*self.error_list.get_children())  
        self.all_errors.clear()

        self.video_path = file_path  
        self.cap = cv2.VideoCapture(self.video_path)

        if not self.cap.isOpened():
            print("Error: Unable to open video file.")
            self.video_message_label.config(text="Error: Cannot open video.", fg="red")
            return

        # Get video properties
        self.fps = self.cap.get(cv2.CAP_PROP_FPS)
        total_frames = self.cap.get(cv2.CAP_PROP_FRAME_COUNT)

        self.video_length = int(total_frames / self.fps) if self.fps > 0 else 0
        video_length_hh_mm_ss = self.convert_seconds_to_hh_mm_ss(self.video_length)

        # Update progress bar and UI
        self.progress_label.config(text=f"Progress: 00:00:00/{video_length_hh_mm_ss} (0%)")
        self.video_message_label.config(text="")  

        print(f"Video uploaded: {self.video_path}")
        print(f"Length: {video_length_hh_mm_ss}, FPS: {self.fps}")

    def start_detection(self):
        if not self.cap or not self.cap.isOpened():
            print("No video uploaded!")
            self.progress_label.config(text="No video uploaded!")
            return
        self.processed_time = 0
        self.detection_paused = False  # Ensure it's not paused at the start
        
        self.simulate_progress()
        self.video_message_label.place_forget()


    def end_detection(self):
        """Stops the detection process and resets the UI."""
        if self.cap:
            self.cap.release()
            self.cap = None

        self.processed_time = 0
        self.progress_bar["value"] = 0
        self.progress_label.config(text="Progress: 00:00:00/00:00:00 (0%)")
        self.error_list.delete(*self.error_list.get_children())  
        self.video_label.config(image="")  

        self.video_message_label.config(text="Please upload video to start detection.", fg="white")
        self.video_message_label.lift(self.video_label)

        print("Detection ended and reset")

    def simulate_progress(self):
        if self.detection_paused:  # Stop frame processing if paused
            return

        ret, frame = self.cap.read()
        if ret:
            self.processed_time += 1
            # Convert processed frames to video time using FPS
            current_time_seconds = self.processed_time / self.fps

            # Update the progress bar percentage
            self.progress_percentage = (current_time_seconds / self.video_length) * 100

            # Display the video frame
            self.display_frame(frame)

            # Call the dropout detection function on the current frame
            self.detect_and_display_errors(frame, current_time_seconds)

            # Update progress bar and label
            current_time_hh_mm_ss = self.convert_seconds_to_hh_mm_ss(int(current_time_seconds))
            video_length_hh_mm_ss = self.convert_seconds_to_hh_mm_ss(self.video_length)
            self.progress_bar["value"] = self.progress_percentage
            self.progress_label.config(
                text=f"Progress: {current_time_hh_mm_ss}/{video_length_hh_mm_ss} ({int(self.progress_percentage)}%)"
            )

            self.root.after(10, self.simulate_progress)  # Update every 10 ms
        else:
            self.progress_label.config(text="Processing Complete!")
            print("Video processing complete.")
            self.cap.release()

    def display_frame(self, frame):
        # Convert the frame to RGB and resize it
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame = cv2.resize(frame, (400, 240))

        # Convert to a format compatible with Tkinter
        img = ImageTk.PhotoImage(Image.fromarray(frame))

        # Update the video label with the new frame
        self.video_label.config(image=img)
        self.video_label.image = img

    def detect_and_display_errors(self, frame, current_time_seconds):
        # Check for black frame
        if checkBlackFrame(frame) == 1:
            if not self.in_black_frame:
                # Start of a new sequence of black frames
                self.in_black_frame = True
                self.first_black_frame_timestamp = self.convert_seconds_to_hh_mm_ss(int(current_time_seconds))
        else:
            # End of black frame sequence
            if self.in_black_frame:
                # Record the error when exiting the sequence
                self.add_error_to_list(self.first_black_frame_timestamp, "Dropout")
            self.in_black_frame = False  # Reset the flag for future detection
            
        # Check for Green Screen
        green_state = checkGreenFrame(frame)
        if green_state == 1:
            timestamp = self.convert_seconds_to_hh_mm_ss(int(current_time_seconds))
            self.add_error_to_list(timestamp, "Green Screen")
        elif green_state == 2:
            timestamp = self.convert_seconds_to_hh_mm_ss(int(current_time_seconds))
            self.add_error_to_list(timestamp, "Mini-map Green Screen")
        
        # Check for Magenta lines
        if checkMagentaFrame(frame):
            # Convert current frame time to hh:mm:ss
            timestamp = self.convert_seconds_to_hh_mm_ss(int(current_time_seconds))
            # Log the error
            self.add_error_to_list(timestamp, "Magenta Lines")
        
        # Check for frozen frames
        if self.previous_frame is not None:
            if detect_frozen_frame(self.previous_frame, frame, threshold=0.1):
                # Log the error
                timestamp = self.convert_seconds_to_hh_mm_ss(int(current_time_seconds))
                self.add_error_to_list(timestamp, "Frozen Frame")


    def add_error_to_list(self, timestamp, error_type):
        self.error_list.insert("", "end", values=(timestamp, error_type))
        print(f"Error Detected: {error_type} at {timestamp}")

    def pause_resume_detection(self):
        if self.detection_paused:
            print("Resuming detection...")
            self.detection_paused = False
            self.simulate_progress()  # Resume frame processing
        else:
            print("Pausing detection...")
            self.detection_paused = True

    def search_errors(self):
        """Filters detected errors based on user input in the search bar."""
        search_term = self.search_entry.get().strip().lower()

        # Clear current treeview content
        self.error_list.delete(*self.error_list.get_children())

        # Filter and display errors matching the search term
        for timestamp, error_type in self.all_errors:
            if search_term in error_type.lower():
                self.error_list.insert("", "end", values=(timestamp, error_type))
    
    def add_error_to_list(self, timestamp, error_type):
        # Add to the master list
        self.all_errors.append((timestamp, error_type))

        # Insert into the Treeview
        self.error_list.insert("", "end", values=(timestamp, error_type))

        print(f"Error Detected: {error_type} at {timestamp}")



    @staticmethod
    def convert_seconds_to_hh_mm_ss(seconds):
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        remaining_seconds = seconds % 60
        return f"{hours:02}:{minutes:02}:{remaining_seconds:02}"


# Dropout detection function 
def checkBlackFrame(frame):
    Black_condition = ~np.any((frame[:, :, 2] > 20) & (frame[:, :, 1] > 20) & (frame[:, :, 0] > 20))
    return 1 if Black_condition else 0

# Green detection function
def checkGreenFrame(frame):
    BRthreshold = 50
    Gthreshold = 100

    miniMapTopLeft = np.array([63, 35])
    miniMapBottomRight = np.array([416, 664])
    miniMapMiddleCord = np.add(miniMapTopLeft, np.subtract(miniMapBottomRight, miniMapTopLeft) // 2)

    height, width = frame.shape[:2]

    # Extract RGB values
    BframeMiddle = frame[height//2, width//2, 0]
    GframeMiddle = frame[height//2, width//2, 1]
    RframeMiddle = frame[height//2, width//2, 2]

    BminiMapMiddle = frame[miniMapMiddleCord[1], miniMapMiddleCord[0], 0]
    GminiMapMiddle = frame[miniMapMiddleCord[1], miniMapMiddleCord[0], 1]
    RminiMapMiddle = frame[miniMapMiddleCord[1], miniMapMiddleCord[0], 2]

    if (BframeMiddle < BRthreshold) and (GframeMiddle > Gthreshold) and (RframeMiddle < BRthreshold):
        return 1  # Normal Green Screen
    elif (BminiMapMiddle < BRthreshold) and (GminiMapMiddle > Gthreshold) and (RminiMapMiddle < BRthreshold):
        return 2  # Mini-map Green Screen
    return 0  # No error

# Magenta detection function
def checkMagentaFrame(frame):
    """
    Returns True if the frame meets the 'magenta' condition, else False.
    Adjust the numeric thresholds if needed.
    """
    # Magenta condition logic
    # Example logic: Red channel > 120, Green channel < 50, Blue channel > 120
    magenta_condition = np.any((frame[..., 2] > 120) & 
                               (frame[..., 1] < 50) & 
                               (frame[..., 0] > 120))

    return bool(magenta_condition)

# Frozen frame detection function
def detect_frozen_frame(frame1, frame2, threshold=0.1):
    """
    Returns True if frame1 and frame2 are similar enough to be considered "frozen."
    threshold: the percentage of differing pixels below which frames are deemed frozen.
    """
    # Convert both frames to grayscale
    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)

    # Calculate absolute difference
    diff = cv2.absdiff(gray1, gray2)

    # Optionally, threshold the difference to ignore minor noise
    # Here, 5 is an arbitrary threshold for pixel intensity difference
    _, diff_thresh = cv2.threshold(diff, 5, 255, cv2.THRESH_BINARY)

    # Count non-zero (differing) pixels
    non_zero_count = np.count_nonzero(diff_thresh)
    total_pixels = diff.shape[0] * diff.shape[1]

    # Ratio of differing pixels
    ratio = non_zero_count / total_pixels

    # If ratio < threshold, we say the frames are "almost identical" => frozen
    return ratio < threshold

# Main Application
if __name__ == "__main__":
    root = tk.Tk()
    app = ErrorDetectionApp(root)
    root.mainloop()
